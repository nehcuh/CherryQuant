#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶

æµ‹è¯•æ–°æ•°æ®ç®¡é“çš„æ€§èƒ½ï¼Œå¹¶ä¸æ—§ç³»ç»Ÿå¯¹æ¯”ã€‚

è¿è¡Œæ–¹å¼ï¼š
    python tests/performance/benchmark_suite.py
"""

import asyncio
import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any
from decimal import Decimal
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from cherryquant.data.pipeline import DataPipeline
from cherryquant.data.collectors.tushare_collector import TushareCollector
from cherryquant.data.collectors.base_collector import Exchange, TimeFrame
from cherryquant.data.query.query_builder import QueryBuilder
from cherryquant.data.query.batch_query import BatchQueryExecutor, BatchQueryRequest
from cherryquant.adapters.data_storage.mongodb_manager import MongoDBConnectionManager


class BenchmarkSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""

    def __init__(self, pipeline: DataPipeline):
        self.pipeline = pipeline
        self.results: Dict[str, List[float]] = {}

    async def setup(self):
        """æµ‹è¯•å‡†å¤‡"""
        print("\n" + "=" * 60)
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        print("\nğŸ“¦ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")

        await self.pipeline.initialize()

        # ç¡®ä¿æœ‰æµ‹è¯•æ•°æ®
        print("ğŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ®...")
        await self._prepare_test_data()

        print("âœ… æµ‹è¯•ç¯å¢ƒå°±ç»ª\n")

    async def teardown(self):
        """æ¸…ç†"""
        print("\nğŸ›‘ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        await self.pipeline.shutdown()

    async def _prepare_test_data(self):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        # ç¡®ä¿æœ‰ rb2501 çš„30å¤©æ•°æ®
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)

        count = await self.pipeline.timeseries_repo.count(
            symbol="rb2501",
            exchange=Exchange.SHFE,
            timeframe=TimeFrame.DAY_1,
            start_date=start_date,
            end_date=end_date,
        )

        if count < 20:
            print(f"  æ•°æ®ä¸è¶³({count}æ¡)ï¼Œå¼€å§‹é‡‡é›†...")
            await self.pipeline.collect_and_store_market_data(
                symbol="rb2501",
                exchange=Exchange.SHFE,
                start_date=start_date,
                end_date=end_date,
                timeframe=TimeFrame.DAY_1,
            )
        else:
            print(f"  æ•°æ®å……è¶³({count}æ¡)")

    def _record_time(self, test_name: str, elapsed: float):
        """è®°å½•æµ‹è¯•æ—¶é—´"""
        if test_name not in self.results:
            self.results[test_name] = []
        self.results[test_name].append(elapsed)

    async def _run_benchmark(
        self,
        name: str,
        func: callable,
        iterations: int = 10,
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•

        Args:
            name: æµ‹è¯•åç§°
            func: æµ‹è¯•å‡½æ•°
            iterations: è¿­ä»£æ¬¡æ•°

        Returns:
            æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"  ğŸ§ª {name}...")

        times = []

        for i in range(iterations):
            start = time.time()
            await func()
            elapsed = time.time() - start
            times.append(elapsed)
            self._record_time(name, elapsed)

        avg_time = statistics.mean(times)
        min_time = min(times)
        max_time = max(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0

        print(f"    å¹³å‡: {avg_time*1000:.2f}ms, æœ€å°: {min_time*1000:.2f}ms, æœ€å¤§: {max_time*1000:.2f}ms")

        return {
            "avg": avg_time,
            "min": min_time,
            "max": max_time,
            "std": std_dev,
            "times": times,
        }

    # ==================== åŸºå‡†æµ‹è¯• ====================

    async def test_simple_query(self):
        """æµ‹è¯•1: ç®€å•æŸ¥è¯¢æ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•1: ç®€å•æŸ¥è¯¢æ€§èƒ½")

        async def query_func():
            data = await self.pipeline.get_market_data(
                symbol="rb2501",
                exchange=Exchange.SHFE,
                start_date=datetime.now() - timedelta(days=7),
                end_date=datetime.now(),
                timeframe=TimeFrame.DAY_1,
                use_cache=False,  # ç¦ç”¨ç¼“å­˜æµ‹è¯•çœŸå®æŸ¥è¯¢æ€§èƒ½
            )
            return len(data)

        stats = await self._run_benchmark("simple_query", query_func)
        return stats

    async def test_cached_query(self):
        """æµ‹è¯•2: ç¼“å­˜æŸ¥è¯¢æ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•2: ç¼“å­˜æŸ¥è¯¢æ€§èƒ½")

        # å…ˆæ‰§è¡Œä¸€æ¬¡å¡«å……ç¼“å­˜
        await self.pipeline.get_market_data(
            symbol="rb2501",
            exchange=Exchange.SHFE,
            start_date=datetime.now() - timedelta(days=7),
            end_date=datetime.now(),
            timeframe=TimeFrame.DAY_1,
            use_cache=True,
        )

        async def query_func():
            data = await self.pipeline.get_market_data(
                symbol="rb2501",
                exchange=Exchange.SHFE,
                start_date=datetime.now() - timedelta(days=7),
                end_date=datetime.now(),
                timeframe=TimeFrame.DAY_1,
                use_cache=True,  # å¯ç”¨ç¼“å­˜
            )
            return len(data)

        stats = await self._run_benchmark("cached_query", query_func)
        return stats

    async def test_query_builder(self):
        """æµ‹è¯•3: QueryBuilder æ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•3: QueryBuilder å¤æ‚æŸ¥è¯¢æ€§èƒ½")

        async def query_func():
            query = (QueryBuilder(self.pipeline.timeseries_repo)
                .symbol("rb2501")
                .exchange(Exchange.SHFE)
                .date_range(
                    datetime.now() - timedelta(days=30),
                    datetime.now()
                )
                .timeframe(TimeFrame.DAY_1)
                .volume_greater_than(10000)
                .price_range(min_price=Decimal("3000"), max_price=Decimal("4000"))
                .limit(20)
            )
            data = await query.execute()
            return len(data)

        stats = await self._run_benchmark("query_builder", query_func)
        return stats

    async def test_batch_query(self):
        """æµ‹è¯•4: æ‰¹é‡æŸ¥è¯¢æ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•4: æ‰¹é‡æŸ¥è¯¢æ€§èƒ½")

        executor = BatchQueryExecutor(
            repository=self.pipeline.timeseries_repo,
            max_concurrency=5,
        )

        async def query_func():
            requests = [
                BatchQueryRequest(
                    symbol="rb2501",
                    exchange=Exchange.SHFE,
                    start_date=datetime.now() - timedelta(days=7),
                    end_date=datetime.now(),
                    timeframe=TimeFrame.DAY_1,
                )
                for _ in range(10)
            ]

            results = await executor.execute_batch(requests)
            return len(results)

        stats = await self._run_benchmark("batch_query", query_func, iterations=5)
        return stats

    async def test_data_collection(self):
        """æµ‹è¯•5: æ•°æ®é‡‡é›†æ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•5: æ•°æ®é‡‡é›†å’Œå­˜å‚¨æ€§èƒ½")

        async def collect_func():
            # é‡‡é›†æœ€è¿‘3å¤©çš„æ•°æ®
            result = await self.pipeline.collect_and_store_market_data(
                symbol="rb2501",
                exchange=Exchange.SHFE,
                start_date=datetime.now() - timedelta(days=3),
                end_date=datetime.now(),
                timeframe=TimeFrame.DAY_1,
                skip_validation=False,
            )
            return result['stored_count']

        stats = await self._run_benchmark("data_collection", collect_func, iterations=3)
        return stats

    async def test_aggregation(self):
        """æµ‹è¯•6: èšåˆæŸ¥è¯¢æ€§èƒ½"""
        print("\nğŸ“Š æµ‹è¯•6: èšåˆæŸ¥è¯¢æ€§èƒ½")

        async def agg_func():
            query = (QueryBuilder(self.pipeline.timeseries_repo)
                .symbol("rb2501")
                .exchange(Exchange.SHFE)
                .date_range(
                    datetime.now() - timedelta(days=30),
                    datetime.now()
                )
                .timeframe(TimeFrame.DAY_1)
            )

            avg_price = await query.avg_price()
            max_price = await query.max_price()
            min_price = await query.min_price()
            total_vol = await query.total_volume()

            return avg_price, max_price, min_price, total_vol

        stats = await self._run_benchmark("aggregation", agg_func)
        return stats

    # ==================== è¿è¡Œæ‰€æœ‰æµ‹è¯• ====================

    async def run_all(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        await self.setup()

        all_stats = {}

        try:
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            all_stats['simple_query'] = await self.test_simple_query()
            all_stats['cached_query'] = await self.test_cached_query()
            all_stats['query_builder'] = await self.test_query_builder()
            all_stats['batch_query'] = await self.test_batch_query()
            all_stats['data_collection'] = await self.test_data_collection()
            all_stats['aggregation'] = await self.test_aggregation()

            # æ‰“å°æ€»ç»“
            self.print_summary(all_stats)

        finally:
            await self.teardown()

        return all_stats

    def print_summary(self, all_stats: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 60)
        print("æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("=" * 60)

        for test_name, stats in all_stats.items():
            print(f"\n{test_name}:")
            print(f"  å¹³å‡æ—¶é—´: {stats['avg']*1000:.2f} ms")
            print(f"  æœ€å°æ—¶é—´: {stats['min']*1000:.2f} ms")
            print(f"  æœ€å¤§æ—¶é—´: {stats['max']*1000:.2f} ms")
            print(f"  æ ‡å‡†å·®: {stats['std']*1000:.2f} ms")

        # æ€§èƒ½å¯¹æ¯”
        print("\n" + "=" * 60)
        print("æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)

        if 'simple_query' in all_stats and 'cached_query' in all_stats:
            speedup = all_stats['simple_query']['avg'] / all_stats['cached_query']['avg']
            print(f"\nç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"  æ— ç¼“å­˜: {all_stats['simple_query']['avg']*1000:.2f} ms")
            print(f"  æœ‰ç¼“å­˜: {all_stats['cached_query']['avg']*1000:.2f} ms")

        print("\n" + "=" * 60)

        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        if self.pipeline.cache:
            print("\nç¼“å­˜ç»Ÿè®¡:")
            self.pipeline.cache.print_stats()


async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("TUSHARE_TOKEN"):
        print("\nâš ï¸  è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡:")
        print("export TUSHARE_TOKEN=your_token_here")
        return

    # åˆå§‹åŒ–æ•°æ®ç®¡é“
    db_manager = MongoDBConnectionManager(
        uri=os.getenv("MONGODB_URI", "mongodb://localhost:27017"),
        database=os.getenv("MONGODB_DATABASE", "cherryquant"),
    )

    collector = TushareCollector(token=os.getenv("TUSHARE_TOKEN"))

    pipeline = DataPipeline(
        collector=collector,
        db_manager=db_manager,
        enable_cache=True,
        enable_validation=True,
        enable_quality_control=True,
    )

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    suite = BenchmarkSuite(pipeline)
    results = await suite.run_all()

    return results


if __name__ == "__main__":
    asyncio.run(main())
