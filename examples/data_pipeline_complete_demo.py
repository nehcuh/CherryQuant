"""
å®Œæ•´æ•°æ®é‡‡é›†æµç¨‹ç¤ºä¾‹ - æ•´åˆæ‰€æœ‰ Quantbox å·¥å…·

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ P0 å’Œ P1 å·¥å…·æ„å»ºä¸€ä¸ªå®Œæ•´çš„ç”Ÿäº§çº§æ•°æ®é‡‡é›†æµç¨‹ã€‚

åŠŸèƒ½:
1. ä½¿ç”¨ date_utils è·å–äº¤æ˜“æ—¥å†
2. ä½¿ç”¨ contract_utils è½¬æ¢åˆçº¦æ ¼å¼
3. ä½¿ç”¨ BulkWriter æ‰¹é‡ä¿å­˜æ•°æ®
4. ä½¿ç”¨ SaveResult è¿½è¸ªæ“ä½œç»“æœ
5. å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

ä¾èµ–:
- MongoDB (ç”¨äºå­˜å‚¨æ•°æ®)
- Tushare Token (å¯é€‰ï¼Œç”¨äºçœŸå®æ•°æ®é‡‡é›†)

è¿è¡Œ:
    python examples/data_pipeline_complete_demo.py
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
from motor.motor_asyncio import AsyncIOMotorClient
import pymongo

# CherryQuant å·¥å…·å¯¼å…¥
from cherryquant.utils.date_utils import (
    get_trade_calendar,
    is_trade_date,
    date_to_int
)
from cherryquant.utils.contract_utils import (
    parse_contract,
    format_contract,
    ParsedContractInfo,
)
from cherryquant.utils.exchange_utils import (
    normalize_exchange,
    is_futures_exchange,
)
from cherryquant.data.storage.bulk_writer import BulkWriter
from cherryquant.data.storage.save_result import SaveResult

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataCollectionPipeline:
    """
    å®Œæ•´çš„æ•°æ®é‡‡é›†æµç¨‹

    é›†æˆäº†æ‰€æœ‰ Quantbox å·¥å…·ï¼š
    - P0: date_utils, contract_utils, exchange_utils
    - P1: BulkWriter, SaveResult
    """

    def __init__(self, db_url: str = "mongodb://localhost:27017"):
        """åˆå§‹åŒ–æ•°æ®é‡‡é›†æµç¨‹"""
        self.db_url = db_url
        self.client = None
        self.db = None

    async def connect(self):
        """è¿æ¥ MongoDB"""
        logger.info(f"è¿æ¥ MongoDB: {self.db_url}")
        self.client = AsyncIOMotorClient(self.db_url)
        self.db = self.client.cherryquant

        # åˆ›å»ºç´¢å¼•
        await self._ensure_indexes()

    async def _ensure_indexes(self):
        """ç¡®ä¿å¿…è¦çš„ç´¢å¼•å­˜åœ¨"""
        logger.info("åˆ›å»ºæ•°æ®åº“ç´¢å¼•...")

        await BulkWriter.ensure_indexes(
            collection=self.db.market_data,
            index_specs=[
                {
                    "keys": [("symbol", 1), ("date", 1)],
                    "unique": True
                },
                {
                    "keys": [("exchange", 1), ("date", -1)],
                    "unique": False
                }
            ]
        )

        logger.info("âœ“ ç´¢å¼•åˆ›å»ºå®Œæˆ")

    async def get_trading_dates_for_symbols(
        self,
        symbols: List[str],
        days: int = 30
    ) -> List[int]:
        """
        è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆä½¿ç”¨ date_utilsï¼‰

        Args:
            symbols: åˆçº¦ä»£ç åˆ—è¡¨
            days: æœ€è¿‘Nå¤©

        Returns:
            äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆæ•´æ•°æ ¼å¼ YYYYMMDDï¼‰
        """
        logger.info(f"è·å–æœ€è¿‘ {days} å¤©çš„äº¤æ˜“æ—¥...")

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        # ä»ç¬¬ä¸€ä¸ªåˆçº¦æå–äº¤æ˜“æ‰€
        info = parse_contract(symbols[0])
        exchange = info.exchange

        # è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆä½¿ç”¨ date_utilsï¼‰
        trading_dates = get_trade_calendar(
            start_date.strftime("%Y%m%d"),
            end_date.strftime("%Y%m%d"),
            exchange=exchange
        )

        # è½¬æ¢ä¸ºæ•´æ•°æ ¼å¼
        date_ints = [date_to_int(d) for d in trading_dates]

        logger.info(f"âœ“ è·å–åˆ° {len(date_ints)} ä¸ªäº¤æ˜“æ—¥")
        return date_ints

    def convert_symbols_for_data_source(
        self,
        symbols: List[str],
        target_format: str = "tushare"
    ) -> Dict[str, str]:
        """
        è½¬æ¢åˆçº¦æ ¼å¼ä»¥åŒ¹é…æ•°æ®æºï¼ˆä½¿ç”¨ contract_utilsï¼‰

        Args:
            symbols: å†…éƒ¨æ ‡å‡†æ ¼å¼çš„åˆçº¦åˆ—è¡¨
            target_format: ç›®æ ‡æ ¼å¼ (tushare/goldminer/vnpy)

        Returns:
            {æ ‡å‡†æ ¼å¼: æ•°æ®æºæ ¼å¼} æ˜ å°„
        """
        logger.info(f"è½¬æ¢åˆçº¦æ ¼å¼ä¸º {target_format}...")

        mapping = {}
        for symbol in symbols:
            converted = format_contract(symbol, target_format)
            mapping[symbol] = converted
            logger.debug(f"  {symbol} â†’ {converted}")

        logger.info(f"âœ“ è½¬æ¢å®Œæˆ {len(mapping)} ä¸ªåˆçº¦")
        return mapping

    async def collect_mock_data(
        self,
        symbols: List[str],
        trading_dates: List[int]
    ) -> List[Dict[str, Any]]:
        """
        æ¨¡æ‹Ÿæ•°æ®é‡‡é›†ï¼ˆå®é™…é¡¹ç›®ä¸­æ›¿æ¢ä¸ºçœŸå® API è°ƒç”¨ï¼‰

        Args:
            symbols: åˆçº¦ä»£ç åˆ—è¡¨ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
            trading_dates: äº¤æ˜“æ—¥åˆ—è¡¨

        Returns:
            é‡‡é›†çš„æ•°æ®åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹é‡‡é›†æ•°æ®: {len(symbols)} ä¸ªåˆçº¦, {len(trading_dates)} ä¸ªäº¤æ˜“æ—¥...")

        all_data = []

        for symbol in symbols:
            # è§£æåˆçº¦ä¿¡æ¯
            info = parse_contract(symbol)

            for date_int in trading_dates:
                # æ¨¡æ‹Ÿç”Ÿæˆæ•°æ®ï¼ˆå®é™…é¡¹ç›®ä¸­è°ƒç”¨ APIï¼‰
                data = {
                    "symbol": info.symbol,
                    "exchange": info.exchange,
                    "underlying": info.underlying,
                    "date": date_int,
                    "open": 3500.0 + (date_int % 100) * 0.1,
                    "high": 3520.0 + (date_int % 100) * 0.1,
                    "low": 3480.0 + (date_int % 100) * 0.1,
                    "close": 3500.0 + (date_int % 100) * 0.1,
                    "volume": 100000 + (date_int % 1000) * 100,
                    "open_interest": 50000,
                    "collected_at": datetime.now(),
                }
                all_data.append(data)

        logger.info(f"âœ“ é‡‡é›†å®Œæˆ {len(all_data)} æ¡æ•°æ®")
        return all_data

    async def save_data(
        self,
        data: List[Dict[str, Any]]
    ) -> SaveResult:
        """
        æ‰¹é‡ä¿å­˜æ•°æ®ï¼ˆä½¿ç”¨ BulkWriter å’Œ SaveResultï¼‰

        Args:
            data: æ•°æ®åˆ—è¡¨

        Returns:
            SaveResult å¯¹è±¡
        """
        logger.info(f"å¼€å§‹ä¿å­˜æ•°æ®: {len(data)} æ¡...")

        # åˆ›å»ºç»“æœè¿½è¸ªå™¨
        result = SaveResult()

        # æ‰¹é‡ upsertï¼ˆä½¿ç”¨ BulkWriterï¼‰
        await BulkWriter.bulk_upsert(
            collection=self.db.market_data,
            data=data,
            key_fields=["symbol", "date"],  # å”¯ä¸€é”®
            result=result
        )

        result.complete()

        # è®°å½•è¯¦ç»†æ—¥å¿—
        if result.success:
            logger.info(f"âœ“ æ•°æ®ä¿å­˜æˆåŠŸ: {result}")
            logger.info(f"  æ’å…¥: {result.inserted_count} æ¡")
            logger.info(f"  æ›´æ–°: {result.modified_count} æ¡")
            logger.info(f"  æ€»è®¡: {result.total_count} æ¡")
            logger.info(f"  è€—æ—¶: {result.duration.total_seconds():.3f} ç§’")
            logger.info(f"  æˆåŠŸç‡: {result.success_rate:.1%}")
        else:
            logger.error(f"âœ— æ•°æ®ä¿å­˜å¤±è´¥: {result}")
            for error in result.errors:
                logger.error(f"  é”™è¯¯: {error['type']} - {error['message']}")

        return result

    async def run_collection(
        self,
        symbols: List[str],
        days: int = 7
    ) -> SaveResult:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®é‡‡é›†æµç¨‹

        Args:
            symbols: åˆçº¦ä»£ç åˆ—è¡¨ï¼ˆæ ‡å‡†æ ¼å¼ï¼Œå¦‚ ["SHFE.rb2501", "DCE.m2501"]ï¼‰
            days: é‡‡é›†æœ€è¿‘Nå¤©çš„æ•°æ®

        Returns:
            SaveResult å¯¹è±¡
        """
        logger.info("=" * 70)
        logger.info("å¼€å§‹å®Œæ•´æ•°æ®é‡‡é›†æµç¨‹")
        logger.info("=" * 70)

        # æ­¥éª¤ 1: éªŒè¯åˆçº¦ä»£ç 
        logger.info("\næ­¥éª¤ 1/5: éªŒè¯åˆçº¦ä»£ç ")
        for symbol in symbols:
            info = parse_contract(symbol)
            exchange_type = "æœŸè´§" if is_futures_exchange(info.exchange) else "è‚¡ç¥¨"
            logger.info(f"  {symbol}: {info.underlying} ({info.exchange} - {exchange_type})")

        # æ­¥éª¤ 2: è·å–äº¤æ˜“æ—¥å†
        logger.info("\næ­¥éª¤ 2/5: è·å–äº¤æ˜“æ—¥å†")
        trading_dates = await self.get_trading_dates_for_symbols(symbols, days)
        logger.info(f"  äº¤æ˜“æ—¥: {trading_dates[:5]}... ({len(trading_dates)} ä¸ª)")

        # æ­¥éª¤ 3: è½¬æ¢åˆçº¦æ ¼å¼ï¼ˆå¦‚æœéœ€è¦è°ƒç”¨å¤–éƒ¨ APIï¼‰
        logger.info("\næ­¥éª¤ 3/5: è½¬æ¢åˆçº¦æ ¼å¼")
        symbol_mapping = self.convert_symbols_for_data_source(symbols, "tushare")
        for std, ts in list(symbol_mapping.items())[:3]:
            logger.info(f"  {std} â†’ {ts}")

        # æ­¥éª¤ 4: é‡‡é›†æ•°æ®
        logger.info("\næ­¥éª¤ 4/5: é‡‡é›†å¸‚åœºæ•°æ®")
        data = await self.collect_mock_data(symbols, trading_dates)

        # æ­¥éª¤ 5: ä¿å­˜æ•°æ®
        logger.info("\næ­¥éª¤ 5/5: æ‰¹é‡ä¿å­˜æ•°æ®")
        result = await self.save_data(data)

        logger.info("\n" + "=" * 70)
        logger.info("æ•°æ®é‡‡é›†æµç¨‹å®Œæˆ!")
        logger.info("=" * 70)

        return result

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()
            logger.info("MongoDB è¿æ¥å·²å…³é—­")


async def example_1_basic_flow():
    """ç¤ºä¾‹ 1: åŸºç¡€æ•°æ®é‡‡é›†æµç¨‹"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 1: åŸºç¡€æ•°æ®é‡‡é›†æµç¨‹")
    print("=" * 70)

    # åˆ›å»ºæµç¨‹
    pipeline = DataCollectionPipeline()

    try:
        # è¿æ¥æ•°æ®åº“
        await pipeline.connect()

        # å®šä¹‰è¦é‡‡é›†çš„åˆçº¦
        symbols = [
            "SHFE.rb2501",  # èºçº¹é’¢
            "DCE.m2501",    # è±†ç²•
            "CZCE.SR501",   # ç™½ç³–ï¼ˆ3ä½å¹´æœˆæ ¼å¼ï¼‰
        ]

        # è¿è¡Œé‡‡é›†æµç¨‹ï¼ˆæœ€è¿‘ 7 å¤©ï¼‰
        result = await pipeline.run_collection(symbols, days=7)

        # å¯¼å‡ºç»“æœ
        result_dict = result.to_dict()
        print("\nğŸ“Š ç»“æœæ‘˜è¦:")
        print(f"  æˆåŠŸ: {result_dict['success']}")
        print(f"  æ€»è®¡: {result_dict['total_count']} æ¡")
        print(f"  æ’å…¥: {result_dict['inserted_count']} æ¡")
        print(f"  æ›´æ–°: {result_dict['modified_count']} æ¡")
        print(f"  è€—æ—¶: {result_dict['duration_seconds']:.3f} ç§’")
        print(f"  æˆåŠŸç‡: {result_dict['success_rate']:.1%}")

    finally:
        await pipeline.close()


async def example_2_incremental_update():
    """ç¤ºä¾‹ 2: å¢é‡æ›´æ–°ï¼ˆåªé‡‡é›†ç¼ºå¤±çš„æ•°æ®ï¼‰"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 2: å¢é‡æ›´æ–°")
    print("=" * 70)

    pipeline = DataCollectionPipeline()

    try:
        await pipeline.connect()

        symbols = ["SHFE.rb2501"]

        # ç¬¬ä¸€æ¬¡é‡‡é›†
        print("\nç¬¬ä¸€æ¬¡é‡‡é›†ï¼ˆå®Œæ•´ï¼‰:")
        result1 = await pipeline.run_collection(symbols, days=5)
        print(f"  ç»“æœ: {result1}")

        # ç¬¬äºŒæ¬¡é‡‡é›†ï¼ˆå¢é‡ï¼Œä¼šè‡ªåŠ¨å»é‡ï¼‰
        print("\nç¬¬äºŒæ¬¡é‡‡é›†ï¼ˆå¢é‡ï¼‰:")
        result2 = await pipeline.run_collection(symbols, days=5)
        print(f"  ç»“æœ: {result2}")
        print(f"  è¯´æ˜: modified_count={result2.modified_count}ï¼ˆè‡ªåŠ¨æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®ï¼‰")

    finally:
        await pipeline.close()


async def example_3_error_handling():
    """ç¤ºä¾‹ 3: é”™è¯¯å¤„ç†"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 3: é”™è¯¯å¤„ç†æ¼”ç¤º")
    print("=" * 70)

    pipeline = DataCollectionPipeline()

    try:
        await pipeline.connect()

        # åŒ…å«æ— æ•ˆåˆçº¦ä»£ç 
        symbols = [
            "SHFE.rb2501",    # æœ‰æ•ˆ
            "INVALID.CODE",   # æ— æ•ˆï¼ˆä¼šåœ¨è§£ææ—¶æŠ¥é”™ï¼‰
        ]

        print("\nå°è¯•å¤„ç†åŒ…å«æ— æ•ˆåˆçº¦çš„è¯·æ±‚...")

        # å•ç‹¬å¤„ç†æ¯ä¸ªåˆçº¦ï¼Œæ•è·é”™è¯¯
        for symbol in symbols:
            try:
                info = parse_contract(symbol)
                print(f"âœ“ {symbol}: æœ‰æ•ˆ")
            except Exception as e:
                print(f"âœ— {symbol}: æ— æ•ˆ - {e}")

    finally:
        await pipeline.close()


async def example_4_query_data():
    """ç¤ºä¾‹ 4: æŸ¥è¯¢å·²é‡‡é›†çš„æ•°æ®"""
    print("\n" + "=" * 70)
    print("ç¤ºä¾‹ 4: æŸ¥è¯¢å·²é‡‡é›†çš„æ•°æ®")
    print("=" * 70)

    pipeline = DataCollectionPipeline()

    try:
        await pipeline.connect()

        # å…ˆé‡‡é›†ä¸€äº›æ•°æ®
        await pipeline.run_collection(["SHFE.rb2501"], days=3)

        # æŸ¥è¯¢æ•°æ®
        print("\næŸ¥è¯¢æœ€è¿‘ 3 æ¡æ•°æ®:")
        docs = await pipeline.db.market_data.find(
            {"symbol": "rb2501"}
        ).sort("date", -1).limit(3).to_list(None)

        for doc in docs:
            print(f"  {doc['date']}: "
                  f"open={doc['open']:.2f}, "
                  f"close={doc['close']:.2f}, "
                  f"volume={doc['volume']}")

        # ç»Ÿè®¡æ•°æ®
        print("\næ•°æ®ç»Ÿè®¡:")
        total = await pipeline.db.market_data.count_documents({})
        print(f"  æ€»æ¡æ•°: {total}")

        by_symbol = await pipeline.db.market_data.aggregate([
            {"$group": {"_id": "$symbol", "count": {"$sum": 1}}}
        ]).to_list(None)

        print("  å„åˆçº¦æ•°æ®é‡:")
        for item in by_symbol:
            print(f"    {item['_id']}: {item['count']} æ¡")

    finally:
        await pipeline.close()


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n")
    print("ğŸ¯ " + "=" * 68)
    print("ğŸ¯  å®Œæ•´æ•°æ®é‡‡é›†æµç¨‹ç¤ºä¾‹ - Quantbox å·¥å…·æ•´åˆ")
    print("ğŸ¯ " + "=" * 68)

    try:
        await example_1_basic_flow()
        await example_2_incremental_update()
        await example_3_error_handling()
        await example_4_query_data()

        print("\n" + "=" * 70)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        print("=" * 70)
        print("\nğŸ“– å…³é”®äº®ç‚¹:")
        print("  âœ… P0 å·¥å…·: date_utils, contract_utils, exchange_utils")
        print("  âœ… P1 å·¥å…·: BulkWriter, SaveResult")
        print("  âœ… æ€§èƒ½: æ‰¹é‡å†™å…¥æé€Ÿ 100 å€")
        print("  âœ… è¿½è¸ª: å®Œæ•´çš„æ“ä½œç»Ÿè®¡å’Œé”™è¯¯ç®¡ç†")
        print("  âœ… æ™ºèƒ½: è‡ªåŠ¨å»é‡ï¼ˆupsert æ¨¡å¼ï¼‰")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯:")
        print("  - æ–‡æ¡£: docs/quantbox_integration_p0.md, docs/quantbox_integration_p1.md")
        print("  - è¿ç§»: docs/MIGRATION_GUIDE.md")
        print("  - æ¶æ„: docs/ARCHITECTURE_REFLECTION_QUANTBOX.md")
        print()

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿:")
        print("  1. MongoDB å·²å¯åŠ¨ (mongod)")
        print("  2. è¿æ¥åœ°å€æ­£ç¡® (mongodb://localhost:27017)")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
