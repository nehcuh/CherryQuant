"""
æ•°æ®ç®¡é“åè°ƒå™¨

æ•´åˆé‡‡é›†ã€æ¸…æ´—ã€å­˜å‚¨ã€æŸ¥è¯¢çš„å®Œæ•´æ•°æ®ç®¡é“ã€‚

æ•™å­¦è¦ç‚¹ï¼š
1. Facade æ¨¡å¼ç®€åŒ–å¤æ‚ç³»ç»Ÿ
2. ä¾èµ–æ³¨å…¥å’Œç»„ä»¶ååŒ
3. ç«¯åˆ°ç«¯çš„æ•°æ®å¤„ç†æµç¨‹
"""

import logging
from typing import Any
from datetime import datetime

from cherryquant.data.collectors.base_collector import (
    BaseCollector,
    MarketData,
    ContractInfo,
    TradingDay,
    Exchange,
    TimeFrame,
)
from cherryquant.data.cleaners.validator import DataValidator
from cherryquant.data.cleaners.normalizer import DataNormalizer
from cherryquant.data.cleaners.quality_control import QualityController
from cherryquant.data.storage.timeseries_repository import TimeSeriesRepository
from cherryquant.data.storage.metadata_repository import MetadataRepository
from cherryquant.data.storage.cache_strategy import CacheStrategy
from cherryquant.data.services.calendar_service import CalendarService
from cherryquant.data.services.contract_service import ContractService
from cherryquant.adapters.data_storage.mongodb_manager import MongoDBConnectionManager

logger = logging.getLogger(__name__)


class DataPipeline:
    """
    æ•°æ®ç®¡é“åè°ƒå™¨

    æä¾›å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š
    é‡‡é›† â†’ éªŒè¯ â†’ æ ‡å‡†åŒ– â†’ è´¨é‡æŽ§åˆ¶ â†’ å­˜å‚¨ â†’ æŸ¥è¯¢

    æ•™å­¦è¦ç‚¹ï¼š
    1. Facade æ¨¡å¼çš„åº”ç”¨
    2. ä¾èµ–æ³¨å…¥å®¹å™¨
    3. ç»„ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†
    """

    def __init__(
        self,
        collector: BaseCollector,
        db_manager: MongoDBConnectionManager,
        enable_cache: bool = True,
        enable_validation: bool = True,
        enable_quality_control: bool = True,
    ):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“

        Args:
            collector: æ•°æ®é‡‡é›†å™¨
            db_manager: æ•°æ®åº“è¿žæŽ¥ç®¡ç†å™¨
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            enable_validation: æ˜¯å¦å¯ç”¨æ•°æ®éªŒè¯
            enable_quality_control: æ˜¯å¦å¯ç”¨è´¨é‡æŽ§åˆ¶

        æ•™å­¦è¦ç‚¹ï¼š
        1. æž„é€ å‡½æ•°æ³¨å…¥
        2. ç‰¹æ€§å¼€å…³ï¼ˆFeature Toggleï¼‰
        """
        self.collector = collector
        self.db_manager = db_manager

        # æ¸…æ´—ç»„ä»¶
        self.validator = DataValidator() if enable_validation else None
        self.normalizer = DataNormalizer()
        self.quality_controller = (
            QualityController(validator=self.validator)
            if enable_quality_control
            else None
        )

        # å­˜å‚¨ç»„ä»¶
        self.timeseries_repo = TimeSeriesRepository(db_manager)
        self.metadata_repo = MetadataRepository(db_manager, enable_cache=enable_cache)

        # ç¼“å­˜ç»„ä»¶
        self.cache = CacheStrategy() if enable_cache else None

        # æœåŠ¡ç»„ä»¶
        self.calendar_service = CalendarService(
            collector=collector,
            repository=self.metadata_repo,
            cache=self.cache,
        )
        self.contract_service = ContractService(
            collector=collector,
            repository=self.metadata_repo,
            validator=self.validator,
            cache=self.cache,
        )

        self._initialized = False

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“

        æ•™å­¦è¦ç‚¹ï¼š
        1. å¼‚æ­¥åˆå§‹åŒ–æ¨¡å¼
        2. èµ„æºé¢„åˆ†é…
        3. å¥åº·æ£€æŸ¥
        """
        if self._initialized:
            logger.info("âœ… æ•°æ®ç®¡é“å·²åˆå§‹åŒ–")
            return

        logger.info("ðŸš€ åˆå§‹åŒ–æ•°æ®ç®¡é“...")

        # 1. è¿žæŽ¥æ•°æ®åº“
        if not self.db_manager._is_connected:
            await self.db_manager.connect()

        # 2. è¿žæŽ¥é‡‡é›†å™¨
        if not self.collector.is_connected:
            await self.collector.connect()

        # 3. åˆ›å»ºç´¢å¼•
        for timeframe in [TimeFrame.MIN_1, TimeFrame.MIN_5, TimeFrame.DAY_1]:
            await self.timeseries_repo.ensure_indexes(timeframe)

        await self.metadata_repo.ensure_indexes()

        self._initialized = True
        logger.info("âœ… æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ")

    async def shutdown(self) -> None:
        """
        å…³é—­æ•°æ®ç®¡é“

        æ•™å­¦è¦ç‚¹ï¼š
        1. ä¼˜é›…å…³é—­
        2. èµ„æºæ¸…ç†
        """
        logger.info("ðŸ›‘ å…³é—­æ•°æ®ç®¡é“...")

        # æ–­å¼€é‡‡é›†å™¨
        if self.collector.is_connected:
            await self.collector.disconnect()

        # æ–­å¼€æ•°æ®åº“
        if self.db_manager._is_connected:
            await self.db_manager.disconnect()

        # æ¸…ç†ç¼“å­˜ç»Ÿè®¡
        if self.cache:
            self.cache.print_stats()

        self._initialized = False
        logger.info("âœ… æ•°æ®ç®¡é“å·²å…³é—­")

    # ==================== å¸‚åœºæ•°æ®ç®¡é“ ====================

    async def collect_and_store_market_data(
        self,
        symbol: str,
        exchange: Exchange,
        start_date: datetime,
        end_date: datetime,
        timeframe: TimeFrame = TimeFrame.DAY_1,
        skip_validation: bool = False,
    ) -> dict[str, Any]:
        """
        é‡‡é›†å¹¶å­˜å‚¨å¸‚åœºæ•°æ®ï¼ˆå®Œæ•´æµç¨‹ï¼‰

        æµç¨‹ï¼šé‡‡é›† â†’ éªŒè¯ â†’ æ ‡å‡†åŒ– â†’ è´¨é‡æŽ§åˆ¶ â†’ å­˜å‚¨

        Args:
            symbol: åˆçº¦ä»£ç 
            exchange: äº¤æ˜“æ‰€
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            timeframe: æ—¶é—´å‘¨æœŸ
            skip_validation: æ˜¯å¦è·³è¿‡éªŒè¯

        Returns:
            Dict: å¤„ç†ç»“æžœç»Ÿè®¡

        æ•™å­¦è¦ç‚¹ï¼š
        1. ç«¯åˆ°ç«¯æ•°æ®å¤„ç†
        2. é”™è¯¯å¤„ç†å’Œé™çº§
        3. å¤„ç†ç»“æžœè¿½è¸ª
        """
        await self._ensure_initialized()

        logger.info(
            f"ðŸ“Š å¼€å§‹æ•°æ®é‡‡é›†: {symbol}.{exchange.value} "
            f"({start_date.date()} åˆ° {end_date.date()}, {timeframe.value})"
        )

        result = {
            "symbol": symbol,
            "exchange": exchange.value,
            "timeframe": timeframe.value,
            "collected_count": 0,
            "valid_count": 0,
            "stored_count": 0,
            "quality_score": 0.0,
            "errors": [],
        }

        try:
            # 1. é‡‡é›†æ•°æ®
            market_data = await self.collector.fetch_market_data(
                symbol=symbol,
                exchange=exchange,
                start_date=start_date,
                end_date=end_date,
                timeframe=timeframe,
            )

            result["collected_count"] = len(market_data)

            if not market_data:
                logger.warning("âš ï¸ æœªé‡‡é›†åˆ°æ•°æ®")
                return result

            # 2. æ•°æ®éªŒè¯ï¼ˆå¯é€‰ï¼‰
            if self.validator and not skip_validation:
                valid_data, invalid_data, validation_result = (
                    self.validator.validate_market_data_batch(market_data)
                )

                result["valid_count"] = len(valid_data)

                if invalid_data:
                    logger.warning(
                        f"âš ï¸ æ•°æ®éªŒè¯: {len(invalid_data)} æ¡æ— æ•ˆæ•°æ®"
                    )
                    result["errors"].append(
                        f"{len(invalid_data)} invalid records"
                    )

                market_data = valid_data

            # 3. æ•°æ®æ ‡å‡†åŒ–
            market_data = self.normalizer.normalize_batch(
                market_data,
                deduplicate=True,
                fill_missing=False,  # å¸‚åœºæ•°æ®ä¸å¡«å……ç¼ºå¤±å€¼
            )

            # 4. è´¨é‡æŽ§åˆ¶ï¼ˆå¯é€‰ï¼‰
            if self.quality_controller:
                quality_metrics = self.quality_controller.assess_data_quality(
                    market_data
                )
                result["quality_score"] = quality_metrics.overall_score

                logger.info(f"ðŸ“Š æ•°æ®è´¨é‡: {quality_metrics.quality_grade}")

            # 5. å­˜å‚¨æ•°æ®
            stored_count = await self.timeseries_repo.save_batch(market_data)
            result["stored_count"] = stored_count

            logger.info(
                f"âœ… æ•°æ®å¤„ç†å®Œæˆ: é‡‡é›† {result['collected_count']}, "
                f"å­˜å‚¨ {stored_count}"
            )

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
            result["errors"].append(str(e))

        return result

    async def get_market_data(
        self,
        symbol: str,
        exchange: Exchange,
        start_date: datetime,
        end_date: datetime,
        timeframe: TimeFrame = TimeFrame.DAY_1,
        use_cache: bool = True,
    ) -> list[MarketData]:
        """
        èŽ·å–å¸‚åœºæ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰

        æ•™å­¦è¦ç‚¹ï¼š
        1. ç¼“å­˜ä¼˜å…ˆç­–ç•¥
        2. æŸ¥è¯¢ä¼˜åŒ–
        """
        await self._ensure_initialized()

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = (
            f"market_data_{symbol}_{exchange.value}_"
            f"{start_date.date()}_{end_date.date()}_{timeframe.value}"
        )

        # å°è¯•ä»Žç¼“å­˜èŽ·å–
        if use_cache and self.cache:
            cached_data = await self.cache.get(cache_key)
            if cached_data:
                logger.debug(f"ðŸ“¦ ç¼“å­˜å‘½ä¸­: {symbol} {timeframe.value}")
                return cached_data

        # ä»Žæ•°æ®åº“æŸ¥è¯¢
        market_data = await self.timeseries_repo.query(
            symbol=symbol,
            exchange=exchange,
            start_date=start_date,
            end_date=end_date,
            timeframe=timeframe,
        )

        # å†™å…¥ç¼“å­˜
        if use_cache and self.cache and market_data:
            await self.cache.set(cache_key, market_data)

        return market_data

    async def get_latest_data(
        self,
        symbol: str,
        exchange: Exchange,
        timeframe: TimeFrame = TimeFrame.DAY_1,
    ) -> MarketData | None:
        """
        èŽ·å–æœ€æ–°çš„ä¸€æ¡æ•°æ®

        æ•™å­¦è¦ç‚¹ï¼š
        1. å•æ¡æŸ¥è¯¢ä¼˜åŒ–
        2. æœ€æ–°æ•°æ®èŽ·å–
        """
        await self._ensure_initialized()

        return await self.timeseries_repo.get_latest(
            symbol=symbol,
            exchange=exchange,
            timeframe=timeframe,
        )

    # ==================== äº¤æ˜“æ—¥åŽ†ç®¡é“ ====================

    async def sync_trading_calendar(
        self,
        exchange: Exchange,
        start_date: datetime,
        end_date: datetime,
    ) -> int:
        """
        åŒæ­¥äº¤æ˜“æ—¥åŽ†ï¼ˆä»£ç†åˆ° CalendarServiceï¼‰

        æ•™å­¦è¦ç‚¹ï¼š
        1. æœåŠ¡ä»£ç†æ¨¡å¼
        2. ç»Ÿä¸€æŽ¥å£
        """
        await self._ensure_initialized()

        return await self.calendar_service.sync_calendar(
            exchange=exchange,
            start_date=start_date,
            end_date=end_date,
        )

    async def is_trading_day(
        self,
        date: datetime,
        exchange: Exchange,
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥"""
        await self._ensure_initialized()

        return await self.calendar_service.is_trading_day(date, exchange)

    async def get_next_trading_day(
        self,
        date: datetime,
        exchange: Exchange,
    ) -> datetime | None:
        """èŽ·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥"""
        await self._ensure_initialized()

        return await self.calendar_service.get_next_trading_day(date, exchange)

    # ==================== åˆçº¦ç®¡ç†ç®¡é“ ====================

    async def sync_contracts(
        self,
        exchange: Exchange | None = None,
    ) -> int:
        """
        åŒæ­¥åˆçº¦ä¿¡æ¯ï¼ˆä»£ç†åˆ° ContractServiceï¼‰
        """
        await self._ensure_initialized()

        return await self.contract_service.sync_contracts(exchange=exchange)

    async def get_contract(
        self,
        symbol: str,
        exchange: Exchange,
    ) -> ContractInfo | None:
        """èŽ·å–åˆçº¦ä¿¡æ¯"""
        await self._ensure_initialized()

        return await self.contract_service.get_contract(symbol, exchange)

    async def get_main_contract(
        self,
        underlying: str,
        exchange: Exchange,
    ) -> ContractInfo | None:
        """èŽ·å–ä¸»åŠ›åˆçº¦"""
        await self._ensure_initialized()

        return await self.contract_service.get_main_contract(underlying, exchange)

    # ==================== æ‰¹é‡æ“ä½œ ====================

    async def batch_collect_and_store(
        self,
        requests: list[dict[str, Any]],
        concurrent_limit: int = 5,
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡é‡‡é›†å’Œå­˜å‚¨æ•°æ®

        Args:
            requests: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚åŒ…å« {symbol, exchange, start_date, end_date, timeframe}
            concurrent_limit: å¹¶å‘é™åˆ¶

        Returns:
            ç»“æžœåˆ—è¡¨

        æ•™å­¦è¦ç‚¹ï¼š
        1. æ‰¹é‡æ“ä½œä¼˜åŒ–
        2. å¹¶å‘æŽ§åˆ¶
        3. ä¿¡å·é‡ï¼ˆSemaphoreï¼‰çš„ä½¿ç”¨
        """
        import asyncio

        await self._ensure_initialized()

        logger.info(f"ðŸ“¦ æ‰¹é‡é‡‡é›†: {len(requests)} ä¸ªè¯·æ±‚")

        # åˆ›å»ºä¿¡å·é‡æŽ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrent_limit)

        async def process_one(request: dict[str, Any]) -> dict[str, Any]:
            async with semaphore:
                return await self.collect_and_store_market_data(**request)

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(
            *[process_one(req) for req in requests],
            return_exceptions=True,
        )

        # ç»Ÿè®¡ç»“æžœ
        success_count = sum(
            1 for r in results
            if isinstance(r, dict) and r.get("stored_count", 0) > 0
        )

        logger.info(f"âœ… æ‰¹é‡é‡‡é›†å®Œæˆ: {success_count}/{len(requests)} æˆåŠŸ")

        return results

    # ==================== æ•°æ®é¢„çƒ­ ====================

    async def warm_up(
        self,
        symbols: list[str],
        exchange: Exchange,
        days_back: int = 30,
        timeframes: list[TimeFrame | None] = None,
    ) -> dict[str, int]:
        """
        æ•°æ®é¢„çƒ­

        é¢„å…ˆåŠ è½½å¸¸ç”¨æ•°æ®åˆ°ç¼“å­˜ï¼Œé¿å…å†·å¯åŠ¨ã€‚

        Args:
            symbols: åˆçº¦ä»£ç åˆ—è¡¨
            exchange: äº¤æ˜“æ‰€
            days_back: å›žæº¯å¤©æ•°
            timeframes: æ—¶é—´å‘¨æœŸåˆ—è¡¨

        Returns:
            é¢„çƒ­ç»“æžœç»Ÿè®¡

        æ•™å­¦è¦ç‚¹ï¼š
        1. ç¼“å­˜é¢„çƒ­ç­–ç•¥
        2. ç³»ç»Ÿå¯åŠ¨ä¼˜åŒ–
        """
        await self._ensure_initialized()

        if not self.cache:
            logger.warning("âš ï¸ ç¼“å­˜æœªå¯ç”¨ï¼Œè·³è¿‡é¢„çƒ­")
            return {}

        timeframes = timeframes or [TimeFrame.DAY_1]
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        logger.info(
            f"ðŸ”¥ å¼€å§‹æ•°æ®é¢„çƒ­: {len(symbols)} ä¸ªåˆçº¦, "
            f"{len(timeframes)} ä¸ªå‘¨æœŸ"
        )

        stats = {
            "total_requests": 0,
            "cache_filled": 0,
        }

        for symbol in symbols:
            for timeframe in timeframes:
                # é¢„åŠ è½½æ•°æ®
                data = await self.get_market_data(
                    symbol=symbol,
                    exchange=exchange,
                    start_date=start_date,
                    end_date=end_date,
                    timeframe=timeframe,
                    use_cache=True,
                )

                stats["total_requests"] += 1
                if data:
                    stats["cache_filled"] += 1

        logger.info(
            f"âœ… æ•°æ®é¢„çƒ­å®Œæˆ: {stats['cache_filled']}/{stats['total_requests']} "
            f"å·²ç¼“å­˜"
        )

        return stats

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    async def _ensure_initialized(self) -> None:
        """ç¡®ä¿ç®¡é“å·²åˆå§‹åŒ–"""
        if not self._initialized:
            await self.initialize()

    def get_stats(self) -> dict[str, Any]:
        """
        èŽ·å–ç®¡é“ç»Ÿè®¡ä¿¡æ¯

        æ•™å­¦è¦ç‚¹ï¼š
        1. ç³»ç»Ÿç›‘æŽ§æŒ‡æ ‡
        2. æ€§èƒ½åˆ†æž
        """
        stats = {
            "initialized": self._initialized,
            "collector": {
                "type": self.collector.__class__.__name__,
                "connected": self.collector.is_connected,
            },
            "database": {
                "connected": self.db_manager._is_connected,
            },
            "components": {
                "validator": self.validator is not None,
                "normalizer": True,
                "quality_controller": self.quality_controller is not None,
                "cache": self.cache is not None,
            },
        }

        # ç¼“å­˜ç»Ÿè®¡
        if self.cache:
            stats["cache"] = self.cache.get_stats()

        return stats

    def print_stats(self) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()

        print("\n" + "=" * 60)
        print("æ•°æ®ç®¡é“ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"çŠ¶æ€: {'å·²åˆå§‹åŒ–' if stats['initialized'] else 'æœªåˆå§‹åŒ–'}")
        print(f"\né‡‡é›†å™¨:")
        print(f"  - ç±»åž‹: {stats['collector']['type']}")
        print(f"  - çŠ¶æ€: {'å·²è¿žæŽ¥' if stats['collector']['connected'] else 'æœªè¿žæŽ¥'}")
        print(f"\næ•°æ®åº“:")
        print(f"  - çŠ¶æ€: {'å·²è¿žæŽ¥' if stats['database']['connected'] else 'æœªè¿žæŽ¥'}")
        print(f"\nç»„ä»¶:")
        for name, enabled in stats['components'].items():
            print(f"  - {name}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

        if "cache" in stats:
            print(f"\nç¼“å­˜ç»Ÿè®¡:")
            cache_stats = stats["cache"]
            print(f"  - L1 å‘½ä¸­çŽ‡: {cache_stats['l1']['hit_rate']}")
            print(f"  - L2 å‘½ä¸­çŽ‡: {cache_stats['l2']['hit_rate']}")
            print(f"  - æ€»è¯·æ±‚æ•°: {cache_stats['total_requests']}")

        print("=" * 60 + "\n")

    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"<DataPipeline("
            f"collector={self.collector.__class__.__name__}, "
            f"initialized={self._initialized})>"
        )
