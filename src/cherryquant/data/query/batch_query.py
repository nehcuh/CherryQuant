"""
æ‰¹é‡æŸ¥è¯¢æ‰§è¡Œå™¨

æä¾›é«˜æ•ˆçš„æ‰¹é‡æŸ¥è¯¢åŠŸèƒ½ï¼Œé€šè¿‡å¹¶å‘æ§åˆ¶å’Œè¿æ¥æ± ä¼˜åŒ–æ€§èƒ½ã€‚

æ•™å­¦è¦ç‚¹ï¼š
1. æ‰¹é‡æ“ä½œæ¨¡å¼
2. å¹¶å‘æ§åˆ¶ï¼ˆSemaphoreï¼‰
3. è¿æ¥æ± ç®¡ç†
4. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
"""

import asyncio
import logging
from typing import Any, Callable
from datetime import datetime
from dataclasses import dataclass

from cherryquant.data.collectors.base_collector import MarketData, Exchange, TimeFrame
from cherryquant.data.storage.timeseries_repository import TimeSeriesRepository

logger = logging.getLogger(__name__)


@dataclass
class BatchQueryRequest:
    """æ‰¹é‡æŸ¥è¯¢è¯·æ±‚"""
    symbol: str
    exchange: Exchange
    start_date: datetime
    end_date: datetime
    timeframe: TimeFrame = TimeFrame.DAY_1
    filters: list[Callable | None] = None


@dataclass
class BatchQueryResult:
    """æ‰¹é‡æŸ¥è¯¢ç»“æœ"""
    request: BatchQueryRequest
    data: list[MarketData]
    success: bool
    error: str | None = None
    execution_time: float = 0.0  # ç§’


class BatchQueryExecutor:
    """
    æ‰¹é‡æŸ¥è¯¢æ‰§è¡Œå™¨

    æä¾›é«˜æ•ˆçš„æ‰¹é‡æŸ¥è¯¢åŠŸèƒ½ã€‚

    æ•™å­¦è¦ç‚¹ï¼š
    1. å¹¶å‘æ§åˆ¶ç­–ç•¥
    2. èµ„æºæ± ç®¡ç†
    3. é”™è¯¯å¤„ç†å’Œé™çº§
    4. æ€§èƒ½ç›‘æ§
    """

    def __init__(
        self,
        repository: TimeSeriesRepository,
        max_concurrency: int = 10,
        timeout: float = 30.0,
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡æŸ¥è¯¢æ‰§è¡Œå™¨

        Args:
            repository: æ—¶é—´åºåˆ—ä»“å‚¨
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            timeout: å•ä¸ªæŸ¥è¯¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        æ•™å­¦è¦ç‚¹ï¼š
        1. å¹¶å‘å‚æ•°é…ç½®
        2. è¶…æ—¶æ§åˆ¶
        """
        self.repository = repository
        self.max_concurrency = max_concurrency
        self.timeout = timeout

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "total_time": 0.0,
        }

    async def execute_batch(
        self,
        requests: list[BatchQueryRequest],
    ) -> list[BatchQueryResult]:
        """
        æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢

        Args:
            requests: æŸ¥è¯¢è¯·æ±‚åˆ—è¡¨

        Returns:
            list[BatchQueryResult]: æŸ¥è¯¢ç»“æœåˆ—è¡¨

        æ•™å­¦è¦ç‚¹ï¼š
        1. ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        2. æ‰¹é‡æ“ä½œæ¨¡å¼
        3. é”™è¯¯éš”ç¦»ï¼ˆä¸€ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–ï¼‰
        """
        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡æŸ¥è¯¢: {len(requests)} ä¸ªè¯·æ±‚")

        start_time = datetime.now()

        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.max_concurrency)

        async def execute_one(request: BatchQueryRequest) -> BatchQueryResult:
            """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢"""
            async with semaphore:
                return await self._execute_single(request)

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        results = await asyncio.gather(
            *[execute_one(req) for req in requests],
            return_exceptions=True,
        )

        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # æŸ¥è¯¢å¤±è´¥ï¼Œåˆ›å»ºé”™è¯¯ç»“æœ
                processed_results.append(
                    BatchQueryResult(
                        request=requests[i],
                        data=[],
                        success=False,
                        error=str(result),
                    )
                )
            else:
                processed_results.append(result)

        # æ›´æ–°ç»Ÿè®¡
        elapsed = (datetime.now() - start_time).total_seconds()
        self.stats["total_queries"] += len(requests)
        self.stats["successful_queries"] += sum(
            1 for r in processed_results if r.success
        )
        self.stats["failed_queries"] += sum(
            1 for r in processed_results if not r.success
        )
        self.stats["total_time"] += elapsed

        logger.info(
            f"âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆ: {self.stats['successful_queries']}/{len(requests)} æˆåŠŸ, "
            f"è€—æ—¶ {elapsed:.2f}s"
        )

        return processed_results

    async def _execute_single(
        self,
        request: BatchQueryRequest,
    ) -> BatchQueryResult:
        """
        æ‰§è¡Œå•ä¸ªæŸ¥è¯¢

        æ•™å­¦è¦ç‚¹ï¼š
        1. è¶…æ—¶æ§åˆ¶
        2. é”™è¯¯å¤„ç†
        3. æ€§èƒ½ç›‘æ§
        """
        start_time = datetime.now()

        try:
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¸¦è¶…æ—¶ï¼‰
            data = await asyncio.wait_for(
                self.repository.query(
                    symbol=request.symbol,
                    exchange=request.exchange,
                    start_date=request.start_date,
                    end_date=request.end_date,
                    timeframe=request.timeframe,
                ),
                timeout=self.timeout,
            )

            # åº”ç”¨è¿‡æ»¤å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if request.filters:
                for filter_func in request.filters:
                    data = [d for d in data if filter_func(d)]

            execution_time = (datetime.now() - start_time).total_seconds()

            return BatchQueryResult(
                request=request,
                data=data,
                success=True,
                execution_time=execution_time,
            )

        except asyncio.TimeoutError:
            logger.warning(
                f"âš ï¸ æŸ¥è¯¢è¶…æ—¶: {request.symbol}.{request.exchange.value}"
            )
            return BatchQueryResult(
                request=request,
                data=[],
                success=False,
                error="Timeout",
                execution_time=self.timeout,
            )

        except Exception as e:
            logger.error(
                f"âŒ æŸ¥è¯¢å¤±è´¥: {request.symbol}.{request.exchange.value} - {e}"
            )
            execution_time = (datetime.now() - start_time).total_seconds()

            return BatchQueryResult(
                request=request,
                data=[],
                success=False,
                error=str(e),
                execution_time=execution_time,
            )

    async def execute_symbols(
        self,
        symbols: list[str],
        exchange: Exchange,
        start_date: datetime,
        end_date: datetime,
        timeframe: TimeFrame = TimeFrame.DAY_1,
    ) -> dict[str, list[MarketData]]:
        """
        æ‰¹é‡æŸ¥è¯¢å¤šä¸ªåˆçº¦çš„æ•°æ®

        Args:
            symbols: åˆçº¦ä»£ç åˆ—è¡¨
            exchange: äº¤æ˜“æ‰€
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            timeframe: æ—¶é—´å‘¨æœŸ

        Returns:
            dict[str, list[MarketData]]: symbol -> æ•°æ®åˆ—è¡¨çš„æ˜ å°„

        æ•™å­¦è¦ç‚¹ï¼š
        1. ä¾¿æ·æ–¹æ³•å°è£…
        2. ç»“æœæ˜ å°„
        """
        # æ„å»ºè¯·æ±‚
        requests = [
            BatchQueryRequest(
                symbol=symbol,
                exchange=exchange,
                start_date=start_date,
                end_date=end_date,
                timeframe=timeframe,
            )
            for symbol in symbols
        ]

        # æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢
        results = await self.execute_batch(requests)

        # æ˜ å°„ç»“æœ
        return {
            result.request.symbol: result.data
            for result in results
            if result.success
        }

    async def execute_timeframes(
        self,
        symbol: str,
        exchange: Exchange,
        start_date: datetime,
        end_date: datetime,
        timeframes: list[TimeFrame],
    ) -> dict[TimeFrame, list[MarketData]]:
        """
        æ‰¹é‡æŸ¥è¯¢åŒä¸€åˆçº¦çš„å¤šä¸ªæ—¶é—´å‘¨æœŸ

        Returns:
            dict[TimeFrame, list[MarketData]]: timeframe -> æ•°æ®åˆ—è¡¨çš„æ˜ å°„

        æ•™å­¦è¦ç‚¹ï¼š
        1. å¤šç»´åº¦æŸ¥è¯¢
        2. ç»“æœç»„ç»‡
        """
        # æ„å»ºè¯·æ±‚
        requests = [
            BatchQueryRequest(
                symbol=symbol,
                exchange=exchange,
                start_date=start_date,
                end_date=end_date,
                timeframe=timeframe,
            )
            for timeframe in timeframes
        ]

        # æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢
        results = await self.execute_batch(requests)

        # æ˜ å°„ç»“æœ
        return {
            result.request.timeframe: result.data
            for result in results
            if result.success
        }

    def get_stats(self) -> dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯

        æ•™å­¦è¦ç‚¹ï¼š
        1. æ€§èƒ½æŒ‡æ ‡è®¡ç®—
        2. ç›‘æ§æ•°æ®æ”¶é›†
        """
        total = self.stats["total_queries"]
        successful = self.stats["successful_queries"]
        failed = self.stats["failed_queries"]
        total_time = self.stats["total_time"]

        return {
            "total_queries": total,
            "successful_queries": successful,
            "failed_queries": failed,
            "success_rate": f"{successful / total * 100:.2f}%" if total > 0 else "N/A",
            "total_time": f"{total_time:.2f}s",
            "avg_time_per_query": f"{total_time / total:.3f}s" if total > 0 else "N/A",
        }

    def print_stats(self) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()

        print("\n" + "=" * 60)
        print("æ‰¹é‡æŸ¥è¯¢ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
        print(f"æˆåŠŸ: {stats['successful_queries']}")
        print(f"å¤±è´¥: {stats['failed_queries']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']}")
        print(f"æ€»è€—æ—¶: {stats['total_time']}")
        print(f"å¹³å‡è€—æ—¶: {stats['avg_time_per_query']}")
        print("=" * 60 + "\n")

    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "total_time": 0.0,
        }
        logger.info("ğŸ“Š æ‰¹é‡æŸ¥è¯¢ç»Ÿè®¡å·²é‡ç½®")
