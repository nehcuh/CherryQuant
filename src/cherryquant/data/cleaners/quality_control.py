"""
æ•°æ®è´¨é‡æ§åˆ¶å™¨

æä¾›æ•°æ®è´¨é‡ç›‘æ§å’ŒæŠ¥å‘ŠåŠŸèƒ½ã€‚

æ•™å­¦è¦ç‚¹ï¼š
1. æ•°æ®è´¨é‡åº¦é‡æŒ‡æ ‡
2. ç›‘æ§å’ŒæŠ¥è­¦æœºåˆ¶
3. è´¨é‡æŠ¥å‘Šç”Ÿæˆ
"""

import logging
from dataclasses import dataclass
from datetime import datetime
from decimal import Decimal

from cherryquant.data.collectors.base_collector import MarketData
from cherryquant.data.cleaners.validator import DataValidator, ValidationResult

logger = logging.getLogger(__name__)


# ==================== è´¨é‡æ§åˆ¶å¸¸é‡é…ç½® ====================

# è´¨é‡è¯„åˆ†æƒé‡
WEIGHT_COMPLETENESS = 0.3  # å®Œæ•´æ€§æƒé‡
WEIGHT_ACCURACY = 0.3      # å‡†ç¡®æ€§æƒé‡
WEIGHT_CONSISTENCY = 0.2   # ä¸€è‡´æ€§æƒé‡
WEIGHT_TIMELINESS = 0.2    # åŠæ—¶æ€§æƒé‡

# è´¨é‡ç­‰çº§é˜ˆå€¼
GRADE_A_THRESHOLD = 0.9    # ä¼˜ç§€
GRADE_B_THRESHOLD = 0.8    # è‰¯å¥½
GRADE_C_THRESHOLD = 0.7    # ä¸­ç­‰
GRADE_D_THRESHOLD = 0.6    # åŠæ ¼

# åŠæ—¶æ€§è¯„åˆ†æ ‡å‡†ï¼ˆå°æ—¶ï¼‰
TIMELINESS_EXCELLENT = 1   # 1å°æ—¶å†…å¾—1.0åˆ†
TIMELINESS_GOOD = 24       # 1å¤©å†…å¾—0.8åˆ†
TIMELINESS_FAIR = 72       # 3å¤©å†…å¾—0.5åˆ†
TIMELINESS_SCORE_EXCELLENT = 1.0
TIMELINESS_SCORE_GOOD = 0.8
TIMELINESS_SCORE_FAIR = 0.5
TIMELINESS_SCORE_POOR = 0.2
TIMELINESS_SCORE_DEFAULT = 0.5  # æ— é‡‡é›†æ—¶é—´æ—¶çš„é»˜è®¤åˆ†æ•°


@dataclass
class QualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    total_count: int                 # æ€»æ•°æ®é‡
    valid_count: int                 # æœ‰æ•ˆæ•°æ®é‡
    invalid_count: int               # æ— æ•ˆæ•°æ®é‡
    error_count: int                 # é”™è¯¯æ•°é‡
    warning_count: int               # è­¦å‘Šæ•°é‡
    completeness_rate: float         # å®Œæ•´æ€§ï¼ˆ0-1ï¼‰
    accuracy_rate: float             # å‡†ç¡®æ€§ï¼ˆ0-1ï¼‰
    consistency_rate: float          # ä¸€è‡´æ€§ï¼ˆ0-1ï¼‰
    timeliness_score: float          # åŠæ—¶æ€§å¾—åˆ†ï¼ˆ0-1ï¼‰

    # å…·ä½“é—®é¢˜ç»Ÿè®¡
    missing_fields: dict[str, int]   # ç¼ºå¤±å­—æ®µç»Ÿè®¡
    outliers_count: int              # ç¦»ç¾¤å€¼æ•°é‡
    duplicates_count: int            # é‡å¤æ•°æ®é‡

    @property
    def overall_score(self) -> float:
        """ç»¼åˆè´¨é‡å¾—åˆ†ï¼ˆ0-1ï¼‰"""
        return (
            self.completeness_rate * WEIGHT_COMPLETENESS +
            self.accuracy_rate * WEIGHT_ACCURACY +
            self.consistency_rate * WEIGHT_CONSISTENCY +
            self.timeliness_score * WEIGHT_TIMELINESS
        )

    @property
    def quality_grade(self) -> str:
        """è´¨é‡ç­‰çº§"""
        score = self.overall_score
        if score >= GRADE_A_THRESHOLD:
            return "ä¼˜ç§€ (A)"
        elif score >= GRADE_B_THRESHOLD:
            return "è‰¯å¥½ (B)"
        elif score >= GRADE_C_THRESHOLD:
            return "ä¸­ç­‰ (C)"
        elif score >= GRADE_D_THRESHOLD:
            return "åŠæ ¼ (D)"
        else:
            return "ä¸åŠæ ¼ (F)"

    def __str__(self) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        return f"""
æ•°æ®è´¨é‡æŠ¥å‘Š
{'=' * 60}
æ€»ä½“æƒ…å†µ:
  - æ€»æ•°æ®é‡: {self.total_count}
  - æœ‰æ•ˆæ•°æ®: {self.valid_count} ({self.valid_count / self.total_count * 100:.1f}%)
  - æ— æ•ˆæ•°æ®: {self.invalid_count} ({self.invalid_count / self.total_count * 100:.1f}%)

é—®é¢˜ç»Ÿè®¡:
  - é”™è¯¯: {self.error_count}
  - è­¦å‘Š: {self.warning_count}
  - ç¦»ç¾¤å€¼: {self.outliers_count}
  - é‡å¤æ•°æ®: {self.duplicates_count}

è´¨é‡æŒ‡æ ‡:
  - å®Œæ•´æ€§: {self.completeness_rate * 100:.1f}%
  - å‡†ç¡®æ€§: {self.accuracy_rate * 100:.1f}%
  - ä¸€è‡´æ€§: {self.consistency_rate * 100:.1f}%
  - åŠæ—¶æ€§: {self.timeliness_score * 100:.1f}%

ç»¼åˆå¾—åˆ†: {self.overall_score * 100:.1f}% - {self.quality_grade}
{'=' * 60}
"""


class QualityController:
    """
    æ•°æ®è´¨é‡æ§åˆ¶å™¨

    æä¾›æ•°æ®è´¨é‡ç›‘æ§ã€è¯„ä¼°å’ŒæŠ¥å‘ŠåŠŸèƒ½ã€‚

    æ•™å­¦è¦ç‚¹ï¼š
    1. è´¨é‡ç®¡ç†çš„ç»´åº¦ï¼ˆå®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ã€åŠæ—¶æ€§ï¼‰
    2. æŒ‡æ ‡è®¡ç®—æ–¹æ³•
    3. è´¨é‡æŠ¥å‘Šçš„è®¾è®¡
    """

    def __init__(
        self,
        validator: DataValidator | None = None,
        min_quality_score: float = 0.7,
    ):
        """
        åˆå§‹åŒ–è´¨é‡æ§åˆ¶å™¨

        Args:
            validator: æ•°æ®éªŒè¯å™¨ï¼ˆå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤ï¼‰
            min_quality_score: æœ€ä½è´¨é‡åˆ†æ•°é˜ˆå€¼
        """
        self.validator = validator or DataValidator()
        self.min_quality_score = min_quality_score

        # å†å²è®°å½•
        self.quality_history: list[QualityMetrics] = []

    def assess_data_quality(
        self,
        data_list: list[MarketData],
    ) -> QualityMetrics:
        """
        è¯„ä¼°æ•°æ®è´¨é‡

        Args:
            data_list: è¦è¯„ä¼°çš„æ•°æ®åˆ—è¡¨

        Returns:
            QualityMetrics: è´¨é‡æŒ‡æ ‡

        æ•™å­¦è¦ç‚¹ï¼š
        1. å¤šç»´åº¦è´¨é‡è¯„ä¼°
        2. æŒ‡æ ‡è®¡ç®—æ–¹æ³•
        3. è´¨é‡åˆ¤å®šæ ‡å‡†
        """
        if not data_list:
            logger.warning("âš ï¸ æ•°æ®åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°è´¨é‡")
            return self._create_empty_metrics()

        # 1. éªŒè¯æ•°æ®
        valid_data, invalid_data, validation_result = (
            self.validator.validate_market_data_batch(data_list)
        )

        # 2. è®¡ç®—å„é¡¹æŒ‡æ ‡
        total_count = len(data_list)
        valid_count = len(valid_data)
        invalid_count = len(invalid_data)

        # å®Œæ•´æ€§ï¼šæœ‰æ•ˆæ•°æ®æ¯”ä¾‹
        completeness_rate = valid_count / total_count if total_count > 0 else 0

        # å‡†ç¡®æ€§ï¼šæ— é”™è¯¯çš„æ•°æ®æ¯”ä¾‹
        error_count = validation_result.summary.get("error", 0)
        accuracy_rate = 1.0 - (error_count / total_count) if total_count > 0 else 0

        # ä¸€è‡´æ€§ï¼šæ— è­¦å‘Šçš„æ•°æ®æ¯”ä¾‹
        warning_count = validation_result.summary.get("warning", 0)
        consistency_rate = 1.0 - (warning_count / total_count) if total_count > 0 else 0

        # åŠæ—¶æ€§ï¼šåŸºäºæ•°æ®æ—¶é—´æˆ³
        timeliness_score = self._calculate_timeliness(data_list)

        # ç¼ºå¤±å­—æ®µç»Ÿè®¡
        missing_fields = self._count_missing_fields(data_list)

        # ç¦»ç¾¤å€¼æ•°é‡ï¼ˆä»éªŒè¯ç»“æœä¸­æå–ï¼‰
        outliers_count = sum(
            1 for issue in validation_result.issues
            if "ç¦»ç¾¤å€¼" in issue.message or "å¼‚å¸¸" in issue.message
        )

        # é‡å¤æ•°æ®ç»Ÿè®¡
        duplicates_count = self._count_duplicates(data_list)

        metrics = QualityMetrics(
            total_count=total_count,
            valid_count=valid_count,
            invalid_count=invalid_count,
            error_count=error_count,
            warning_count=warning_count,
            completeness_rate=completeness_rate,
            accuracy_rate=accuracy_rate,
            consistency_rate=consistency_rate,
            timeliness_score=timeliness_score,
            missing_fields=missing_fields,
            outliers_count=outliers_count,
            duplicates_count=duplicates_count,
        )

        # è®°å½•å†å²
        self.quality_history.append(metrics)

        # æ—¥å¿—è¾“å‡º
        logger.info(f"ğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆ: {metrics.quality_grade}")
        if metrics.overall_score < self.min_quality_score:
            logger.warning(
                f"âš ï¸ æ•°æ®è´¨é‡ä½äºé˜ˆå€¼ "
                f"({metrics.overall_score:.2f} < {self.min_quality_score})"
            )

        return metrics

    def _calculate_timeliness(self, data_list: list[MarketData]) -> float:
        """
        è®¡ç®—æ•°æ®åŠæ—¶æ€§å¾—åˆ†

        æ•™å­¦è¦ç‚¹ï¼š
        1. åŠæ—¶æ€§çš„å®šä¹‰
        2. æ—¶é—´è¡°å‡å‡½æ•°
        3. å®æ—¶æ•°æ® vs å†å²æ•°æ®çš„åŒºåˆ«
        """
        if not data_list:
            return 0.0

        now = datetime.now()
        scores = []

        for data in data_list:
            if not data.collected_at:
                # å¦‚æœæ²¡æœ‰é‡‡é›†æ—¶é—´ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°
                scores.append(TIMELINESS_SCORE_DEFAULT)
                continue

            # è®¡ç®—æ•°æ®å»¶è¿Ÿï¼ˆé‡‡é›†æ—¶é—´ - æ•°æ®æ—¶é—´ï¼‰
            delay = (data.collected_at - data.datetime).total_seconds()

            # æ—¶æ•ˆæ€§å¾—åˆ†ï¼šå»¶è¿Ÿè¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°
            if delay < TIMELINESS_EXCELLENT * 3600:  # 1å°æ—¶
                score = TIMELINESS_SCORE_EXCELLENT
            elif delay < TIMELINESS_GOOD * 3600:  # 1å¤©
                score = TIMELINESS_SCORE_GOOD
            elif delay < TIMELINESS_FAIR * 3600:  # 3å¤©
                score = TIMELINESS_SCORE_FAIR
            else:
                score = TIMELINESS_SCORE_POOR

            scores.append(score)

        return sum(scores) / len(scores) if scores else 0.0

    def _count_missing_fields(
        self,
        data_list: list[MarketData],
    ) -> dict[str, int]:
        """ç»Ÿè®¡ç¼ºå¤±å­—æ®µ"""
        missing_counts = {
            "open_interest": 0,
            "turnover": 0,
            "collected_at": 0,
        }

        for data in data_list:
            if data.open_interest is None:
                missing_counts["open_interest"] += 1
            if data.turnover is None:
                missing_counts["turnover"] += 1
            if data.collected_at is None:
                missing_counts["collected_at"] += 1

        return missing_counts

    def _count_duplicates(self, data_list: list[MarketData]) -> int:
        """ç»Ÿè®¡é‡å¤æ•°æ®"""
        seen = set()
        duplicates = 0

        for data in data_list:
            key = (
                data.symbol,
                data.exchange.value,
                data.datetime,
                data.timeframe.value,
            )

            if key in seen:
                duplicates += 1
            else:
                seen.add(key)

        return duplicates

    def _create_empty_metrics(self) -> QualityMetrics:
        """åˆ›å»ºç©ºçš„è´¨é‡æŒ‡æ ‡"""
        return QualityMetrics(
            total_count=0,
            valid_count=0,
            invalid_count=0,
            error_count=0,
            warning_count=0,
            completeness_rate=0.0,
            accuracy_rate=0.0,
            consistency_rate=0.0,
            timeliness_score=0.0,
            missing_fields={},
            outliers_count=0,
            duplicates_count=0,
        )

    def generate_quality_report(
        self,
        metrics: QualityMetrics,
        output_file: str | None = None,
    ) -> str:
        """
        ç”Ÿæˆè´¨é‡æŠ¥å‘Š

        Args:
            metrics: è´¨é‡æŒ‡æ ‡
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: æŠ¥å‘Šå†…å®¹

        æ•™å­¦è¦ç‚¹ï¼š
        1. æŠ¥å‘Šæ ¼å¼è®¾è®¡
        2. å¯è§†åŒ–è¡¨ç¤º
        3. æ–‡ä»¶è¾“å‡º
        """
        report = str(metrics)

        # æ·»åŠ è¯¦ç»†çš„ç¼ºå¤±å­—æ®µä¿¡æ¯
        if metrics.missing_fields:
            report += "\nç¼ºå¤±å­—æ®µè¯¦æƒ…:\n"
            for field, count in metrics.missing_fields.items():
                rate = count / metrics.total_count * 100 if metrics.total_count > 0 else 0
                report += f"  - {field}: {count} ({rate:.1f}%)\n"

        # æ·»åŠ å»ºè®®
        report += "\næ”¹è¿›å»ºè®®:\n"
        if metrics.completeness_rate < 0.9:
            report += "  - æé«˜æ•°æ®å®Œæ•´æ€§ï¼šæ£€æŸ¥æ•°æ®é‡‡é›†æµç¨‹\n"
        if metrics.accuracy_rate < 0.9:
            report += "  - æé«˜æ•°æ®å‡†ç¡®æ€§ï¼šåŠ å¼ºæ•°æ®éªŒè¯\n"
        if metrics.consistency_rate < 0.9:
            report += "  - æé«˜æ•°æ®ä¸€è‡´æ€§ï¼šæ£€æŸ¥ OHLC å…³ç³»\n"
        if metrics.timeliness_score < 0.8:
            report += "  - æé«˜æ•°æ®åŠæ—¶æ€§ï¼šä¼˜åŒ–é‡‡é›†é¢‘ç‡\n"
        if metrics.duplicates_count > 0:
            report += "  - å»é™¤é‡å¤æ•°æ®ï¼šå¯ç”¨å»é‡æœºåˆ¶\n"

        # è¾“å‡ºåˆ°æ–‡ä»¶
        if output_file:
            try:
                with open(output_file, "w", encoding="utf-8") as f:
                    f.write(report)
                logger.info(f"âœ… è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

        return report

    def get_quality_trend(self, last_n: int = 10) -> list[float]:
        """
        è·å–è´¨é‡è¶‹åŠ¿

        Args:
            last_n: æœ€è¿‘Næ¬¡è¯„ä¼°

        Returns:
            list[float]: è´¨é‡å¾—åˆ†åˆ—è¡¨

        æ•™å­¦è¦ç‚¹ï¼š
        1. æ—¶é—´åºåˆ—åˆ†æ
        2. è¶‹åŠ¿è¯†åˆ«
        3. å†å²æ•°æ®åˆ©ç”¨
        """
        if not self.quality_history:
            return []

        recent = self.quality_history[-last_n:]
        return [m.overall_score for m in recent]

    def is_quality_degrading(self, window: int = 5) -> bool:
        """
        åˆ¤æ–­è´¨é‡æ˜¯å¦åœ¨ä¸‹é™

        Args:
            window: è§‚å¯Ÿçª—å£å¤§å°

        Returns:
            bool: è´¨é‡æ˜¯å¦åœ¨ä¸‹é™

        æ•™å­¦è¦ç‚¹ï¼š
        1. è¶‹åŠ¿æ£€æµ‹ç®—æ³•
        2. æ—©æœŸé¢„è­¦æœºåˆ¶
        """
        trend = self.get_quality_trend(window)

        if len(trend) < 2:
            return False

        # ç®€å•çš„çº¿æ€§å›å½’åˆ¤æ–­è¶‹åŠ¿
        # å¦‚æœååŠéƒ¨åˆ†å‡å€¼ < å‰åŠéƒ¨åˆ†å‡å€¼ï¼Œè®¤ä¸ºåœ¨ä¸‹é™
        mid = len(trend) // 2
        first_half = sum(trend[:mid]) / mid if mid > 0 else 0
        second_half = sum(trend[mid:]) / (len(trend) - mid) if len(trend) > mid else 0

        return second_half < first_half * 0.95  # ä¸‹é™è¶…è¿‡5%
